{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buenas prácticas para el desarrollo de aplicaciones\n",
    "\n",
    "La infraestructura que proporciona OpenAI para el desarrollo de aplicaciones y las llamadas a la API que corresponde a este uso es fuerte y una de las primeras cosas que pide OpenAI es que tanto durante el desarrollo como durante la producción esta se utilice de forma responsable y reflexiva.\n",
    "\n",
    "Dado el estado de la tecnología, controlan bastante las etapas de desarrollo, se interesan por el uso y las posibles aplicaciones que se le pueden dar a la API, para las que, en su mayor parte, hay que solicitar una revisión y aprobación del uso por parte de la compañía. La escalabilidad en el acceso al modelo busca, según la guía para desarrolladores, dar libertad creativa a la hora de construir y desplegar aplicaciones a la vez que les permite estudiar y determinar el impacto potencial de la tecnología y la aplicación.\n",
    "\n",
    "Uno de los avisos que más recalcan es que, a la hora de planificar el lanzamiento de una aplicación que utilice GPT-3, **OpenAI deberá revisar el proyecto y cómo se ha integrado la API para evaluar los riesgos potenciales, el crecimiento en el volumen de peticiones que hará la aplicación y evaluar los posibles usos indebidos.**\n",
    "\n",
    "Para tener muy en cuenta, OpenAI considera que estás sirviendo la API cuando hay más de 5 personas implicadas; es decir, no solo afecta cuando abres la aplicación al público general sino también contaría cuando abres el uso a un equipo de más de 5 personas en una empresa o pruebas la beta de la aplicación con usuarios externos. **En el caso de hacerlo sin presentar una revisión, la clave de API podría ser revocada e incluso perdida de forma permanente.**\n",
    "\n",
    "\n",
    "## Ciclo de vida y proceso de desarrollo\n",
    "\n",
    "OpenAI describe en su documentación un ciclo de vida en el desarrollo de aplicaciones en 6 pasos:\n",
    "\n",
    "1. **Revisión de las directrices de los casos de uso** para confirmar que el de la aplicación está admitido dentro de los que OpenAI permite.\n",
    "2. **Desarrolla la aplicación o producto** siguiendo las buenas prácticas de seguridad.\n",
    "3. **Cuando el producto esté listo para el lanzamiento**, solicite la revisión previa a OpenAI.\n",
    "4. **Revisión de OpenAI del proyecto** para evaluar los riesgos, estudiar el crecimiento y la supervisión de uso inadecuado.\n",
    "5. **Aprobación** o desestimado **de la aplicación**.\n",
    "6. **Solicitudes de aumento de la cuota** según sea necesaria teniendo en cuenta el crecimiento y el acumulado del historial.\n",
    "\n",
    "\n",
    "## Buenas prácticas de seguridad\n",
    "\n",
    "Igual que en cualquier proyecto, definir unas buenas prácticas ayudará a que este se desarrolle de forma óptima. Para el uso de GPT-3 en producción, OpenAI pone a disposición de los desarrolladores unas pautas que deben seguirse a la hora de utilizar el modelo. Ayuda a depurar errores, implementar sin realizar malas prácticas, sean intencionadas o no.\n",
    "\n",
    "Además, estas prácticas deben aplicarse desde el principio del desarrollo, teniendo siempre presente que la aplicación será revisada en el prelanzamiento y podrá detectarse que no se han tenido en cuenta, lo que podría suponer un retraso en el lanzamiento al no considerarse segura. Para ello, han escrito una **lista de 10 recomendaciones cuando se está trabajando en una aplicación de alto nivel**.\n",
    "\n",
    "1. **Evita que estas recomendaciones te desanimen**. El objetivo de estas es que los desarrolladores construyan y experimenten con la API, entendemos que lo harán utilizando un criterio objetivo y bueno, guiado por estas directrices. En el caso de encontrar un caso de uso que no estás seguro de que esté dentro de los casos aceptados, puedes participar en las lluvias de idea de la comunidad o contactar con el personal de OpenAI que se dedica a Riesgo y seguridad para resolverlo. Muchas de las aplicaciones con más éxito al principio no parecían seguras, una parte importante del desarrollo es buscar soluciones y trabajar en reducir el riesgo de uso indebido.\n",
    "\n",
    "2. **Piensa como el enemigo**. Aunque el producto que desarrollas está pensado de una forma específica, piensa en los usos potenciales que pueden darse de la aplicación. La planificación de estos riesgos es necesaria y útil para reducir la probabilidad de un uso inesperado o inadecuado por parte de alguno de los usuarios de la aplicación final. Es muy recomendable generar un equipo de probadores que traten de descubrir las debilidades de la aplicación y los usos fraudulentos a los que pueda tener acceso a través de ella.\n",
    "\n",
    "3. **Limita la longitud de las solicitudes y respuestas de la API**. Dependiendo del uso y la intención de la aplicación, estos límites deberán calcularse de una manera u otra. De forma genérica, podría aplicarse la siguiente regla para limitar el contenido solicitado y suministrado: nº de caracteres * 100 / 4, lo que aproximará la entrada del usuario. En el caso de las palabras de salida, se puede establecer con el parámetro *max_tokens* de la API.\n",
    "\n",
    "4. **Estudio de clientes**. Conocer a los clientes es fundamental y exigir una autentificación de usuarios permitirá realizar controles de uso indebido y permitirá intervenir adecuadamente si se detectase. Ten en cuenta el caso de uso, ya que la autentificación de usuarios deberá ajustarse al nivel de seguridad que requiera la aplicación; puede que para ciertos usos no sea suficiente con la existencia de un vínculo con una cuenta de correo electrónico o red social; sino que tal vez sería recomendable un vínculo a método de pago para acceder a la aplicación (lo que aumentaría la dificultad de tener *clientes maliciosos*).\n",
    "    En el caso de valorar la creación de una prueba gratuita, esta debe estar correctamente limitada para que no se convierta en un agujero de seguridad.\n",
    "\n",
    "5. **Limitar el tiempo de uso**. Los límites de uso permiten reducir la probabilidad de mal uso y limita la exposición a problemas de seguridad y el incremento del coste. Puedes tener limitaciones por tiempo determinado, utilizando un límite estricto o establecer puntos de control manuales; hay que tener en cuenta el uso habitual que hará el usuario medio y, a partir de él, establecer la forma en la que se limite para evitar los usos indebidos sin causar prejuicios a un incremento puntual del uso de un usuario. Como alternativas al tiempo de uso, se puede trabajar:\n",
    "    - Crear un tiempo de restricción entre diferentes llamadas a la API por parte de un usuario en particular, reduciendo el uso automatizado.\n",
    "    - Crear una limitación de uso a través del número de dirección IP, evitando que este realice peticiones simultaneas.\n",
    "    \n",
    "6. **Filtrar el contenido sensible e inseguro**. Crear y utilizar filtros para garantizar que las aplicaciones se comporten de forma correcta, sin comportamientos inesperados o inadecuados. El filtro que proporciona OpenAI es gratuito y puede ayudar a reducir la frecuencia de contenido sensible en los textos generados por GPT-3. Como medidas adicionales, se pueden implementar en la API herramientas de control que hagan este trabajo sobre las salidas del modelo, las identifique mediante reglas creadas con regex u otros sistemas. También recurriendo a rutinas que permitan que el usuario no note el filtro, como la sustitución de palabrotas o sistemas controlados de asistentes virtuales basados en sistemas de pregunta y respuesta predefinidos que inviten al usuario a generar una nueva interacción.\n",
    "\n",
    "7. **Mantener la aplicación dentro de su definición**. Las aplicaciones que se desvían fácilmente de su uso previsto pueden suponer un riesgo de mal uso. Los sistemas de avisos puede ayudar a restringir el tema y el tono del texto de salida.\n",
    "\n",
    "8. **Capturar el uso de la aplicación**. Tener una retroalimentación de los usuarios ayudará a que la aplicación se mantenga en estándares de calidad adecuados y permite tener una información adicional sobre la funcionalidad y comportamiento de la aplicación. Estos recursos deben ser administrados y evaluados por humanos, ya que estos sistemas detectarán usos inadecuados y situaciones que deben resolverse; para estos casos, es vital la supervisión de los canales de redes sociales relacionados con el producto, para detectar posibles usos indebidos o comportamientos inadecuados, esto tendrá que comunicarse, además, al personal de OpenAI en cuanto se detecte (los métodos habituales son el contacto con el servicio de asistencia técnica a través del chat *in situ* o en support@OpenAI.com). Además, es importante que los usuarios conozcan la existencia de los mecanismos y que se animen a participar en ellos de forma activa, por ejemplo, introduciendo algún sistema de gamificación.\n",
    "\n",
    "9. **Mantener al humano en el proceso**. Mantener el factor humano involucrado en el desarrollo asegura que los incidentes graves se puedan detectar, abordar y que puedan establecerse expectativas apropiadas, tanto para la IA como para el humano. Es vital indicar de forma clara qué es lo que puede realizar la IA y qué es lo que hace el humano dentro de la aplicación, especialmente en las interacciones iniciales del usuario. Es importante también especificar y dejar claras cuales son las líneas rojas y la definición de uso inadecuado. En general, los usuarios no deberían crear flujos de trabajo automatizados en torno a la API sin que un humano ejerza su juicio como parte del proceso.\n",
    "\n",
    "10. **Usa contenido validado**. Limitar los rangos de las entradas o salidas, especialmente las extraídas de fuentes de confianza, reduce el alcance del mal uso posible dentro de una aplicación. Permitir las entradas del usuario con campos desplegables puede ser más seguro que permitir entradas de texto abiertas.\n",
    "\n",
    "\n",
    "**Como para OpenAI la seguridad es un pilar importante, si durante el desarrollo de la aplicación se detectan problemas de seguridad con la API, es importante hacerle llegar la información dentro del programa coordinado de divulgación de vulnerabilidades ([Coordinated Vulnerability Disclosure Program](https://OpenAI.com/security/disclosure/))**.\n",
    "\n",
    "\n",
    "## Política de riesgos y seguridad de OpenAI para el uso de la API en producción\n",
    "\n",
    "Aunque la API pretende generar beneficios para los clientes y usuarios finales, también es una potencial fuente de problemas de seguridad que es importante caracterizar y mitigar. Para ello, OpenAI tiene en su documentación un apartado de mejores prácticas para el uso de GPT-3 que permite al usuario comprender sus riesgos de uso.\n",
    "\n",
    "Este resumen puede ampliarse en la [documentación que facilitan en inglés](https://beta.OpenAI.com/docs/safety-best-practices/understanding-safety-risks) y, de no ser un recurso suficiente, también se puede contactar con ellos a través de support@OpenAI.com para resolver dudas.\n",
    "\n",
    "#### Retos de seguridad para los sistemas de aprendizaje automático abiertos y de gran ancho de banda\n",
    "\n",
    "Para la API de GPT-3, OpenAI adapta una versión modificada y más amplia de la definición de seguridad general:\n",
    "\n",
    "```\n",
    "Estar libre de condiciones que puedan causar daños físicos, psicológicos o sociales a las personas, incluyendo, pero sin limitarse a ello, la muerte, las lesiones, las enfermedades, la angustia, la desinformación o la radicalización, el daño o la pérdida de la vivienda o de la oportunidad, o el daño al medio ambiente.\n",
    "```\n",
    "\n",
    "El lenguaje natural permite una comunicación abierta y de gran variabilidad, lo que hace necesario implementar ciertas medidas de seguridad de la API que consideren este tipo de sistemas de aprendizaje automático con una baja robustez. Estos sistemas podrían sufrir interacciones inapropiadas, ya que aunque los usuarios traten con condiciones similares al entrenamiento, estos pueden añadir información insegura o, incluso, recibir entradas maliciosas que deliberadamente tratan de encontrar una respuesta no deseada del sistema.\n",
    "\n",
    "Para mitigar esto, los humanos deben ser los que evalúen los resultados del modelo para cada caso considerado, generando una guía de resultados deseables a través de un volumen de entradas representativo, teniendo en cuenta también las adversas.\n",
    "\n",
    "Hay que tener en cuenta también los sesgos, ya que los componentes del aprendizaje automático reflejan los valores presentes en los datos de entrenamiento y de los propios desarrolladores. Hay que mantener una vigilancia activa sobre estos factores, sobre todo en las aplicaciones que dejan interactuar libremente al modelo con el usuario para detectar y evitar que perpetúen o amplifiquen estos comportamientos.\n",
    "\n",
    "Cuanto más abierto esté el producto final a la interacción del usuario, los riesgos que se corren de encontrarse con estos sesgos o respuestas inadecuadas aumenta, por lo que hay que tener en cuenta que los asistentes virtuales que utilizan GPT-3 son susceptibles de, aunque se espere una respuesta determinada o se usen con un propósito determinado, se acabarán utilizando de forma más abierta y general por parte de los usuario.\n",
    "\n",
    "Para evitar este tipo de comportamientos, OpenAI recomienda un enfoque centrado en la consideración de amplias categorías y contextos de daños potenciales, la detección y respuesta continuas a los incidentes de daños, y la integración continua de nuevas mitigaciones a medida que las necesidades se hacen evidentes.\n",
    "\n",
    "Hay que tener en cuenta, además, que las características de seguridad cambian cada vez que se actualizan los componentes (si se vuelve a entrenar con nuevos datos o si se entrenan nuevos componentes desde cero con arquitecturas novedosas). Este área de desarrollo permanece en investigación activa y que a medida que avanza se van desbloqueando nuevos niveles de rendimiento; es recomendable que los diseñadores se puedan anticipar a estas actualizaciones e incidencias que puedan ocurrir y que realicen una planificación sobre los análisis de seguridad recurrentes que deberán llevar a cabo.\n",
    "\n",
    "#### Perjuicios en el análisis de riesgos\n",
    "\n",
    "Para facilitar la comprensión de los riesgos que deben afrontarse cuando se desarrolla un producto con Inteligencia Artificial, OpenAI ofrece unos ejemplos de posibles daños que pueden surgir con el uso de GPT-3.\n",
    "Uno de los principales recursos para evitarlo es la consideración exhaustiva de los sistemas desarrollados en su contexto, incluyendo los sistemas que están sujetos a este y a las posibles fuentes dañinas que podrían acceder mediante estos. Los ejemplos que proporciona OpenAI, aunque existen más casos, son:\n",
    "\n",
    "- **Proporcionar información falsa**. Prohíben, como es lógico, el uso de la API para producir y difundir intencionadamente información engañosa.\n",
    "\n",
    "- **Perpetuar actitudes discriminatorias**. El sistema puede persuadir a los usuarios para que crean cosas perjudiciales sobre grupos de personas, por ejemplo, utilizando un lenguaje racista, sexista o incapacitante.\n",
    "\n",
    "- **Angustia individual**. El sistema puede crear resultados que sean angustiosos para un individuo, por ejemplo, fomentando un comportamiento autodestructivo o dañando su autoestima.\n",
    "\n",
    "- **Incitación a la violencia**. El sistema puede persuadir a los usuarios para que adopten un comportamiento violento contra cualquier otra persona o grupo.\n",
    "\n",
    "- **Lesiones físicas, daños a la vivienda o al medio ambiente**. En algunos casos de uso, si la aplicación que utiliza la API está conectado a actuadores físicos con el potencial de causar daños, el sistema es crítico para la seguridad, y podrían producirse fallos físicamente perjudiciales debido a un comportamiento imprevisto de la API.\n",
    "\n",
    "Hay que tener presente que GPT-3 no tiene conciencia del bien y del mal, por lo tanto, hay que tener cuidado con los juicios y con los desarrollos que realiza, ya que podrían considerarse contenidos altamente amorales o inmorales.\n",
    "\n",
    "#### La importancia de la robustez\n",
    "\n",
    "La robustez es un reto.\n",
    "\n",
    "***Robustez*** significa que una aplicación funcione tal y como se pretende y se espera en un contexto determinado. Los clientes de la API deben asegurarse de que su aplicación es tan robusta como se requiere para un uso seguro y deben asegurarse de que dicha robustez se mantiene en el tiempo.\n",
    "\n",
    "Los modelos lingüísticos son útiles para una serie de propósitos, pero pueden fallar de forma inesperada debido al conocimiento limitado del mundo que se les ha introducido. Estos fallos pueden ser visibles, como la generación de un texto irrelevante u obviamente incorrecto, o invisibles, como el hecho de no encontrar un resultado relevante.\n",
    "\n",
    "Los riesgos asociados al uso de la API variarán sustancialmente en función de los casos de uso, como por ejemplo:\n",
    "\n",
    "- Generar texto irrelevante para el contexto\n",
    "- Generar texto inexacto debido a una laguna en el conocimiento del mundo de la API\n",
    "- Creación de contexto ofensivo\n",
    "- Clasificación erróneas del texto al no comprender correctamente el contexto\n",
    "\n",
    "**El contexto importa**. Los clientes deben tener en cuenta que los resultados de la API dependen en gran medida del contexto proporcionado al modelo; es enriquecedor generar contextos adicionales que ayuden a mejorar la calidad del comportamiento deseado antes de que se genere la salida.\n",
    "\n",
    "Lo que nos lleva a la necesidad de **fomentar la supervisión humana**. Incluso con esfuerzos sustanciales para aumentar la robustez, es probable que se produzcan algunos fallos. Lo que hace fundamental que haya interacción y comunicación a través de toda la cadena: de los usuarios finales a los desarrolladores y de los desarrolladores de la aplicación con OpenAI.\n",
    "\n",
    "**Seguir probando**. La verificación de la seguridad y la eficacia del modelo a la hora de generar respuestas y ajustarse al cometido debe hacerse a lo largo del tiempo ya que la robustez puede variar a lo largo del tiempo o, incluso, cambiar si en algún futuro OpenAI proporciona versiones mejoradas de los modelos, cosa que podría requerir de reajustes, validaciones y evaluaciones.\n",
    "\n",
    "#### 5. La importancia de la equidad\n",
    "***Equidad*** implica que la API no tenga un rendimiento menor para los usuarios en función de su demografía, ni produzca un texto que sea perjudicial para determinados grupos demográficos. Los clientes de la API deben tomar medidas razonables para identificar y reducir los daños previsibles asociados a los sesgos demográficos en la API.\n",
    "\n",
    "Actualmente, el desafío en el desarrollo de aplicaciones se encuentra en encontrar sistemas imparciales que sean capaces de eliminar los sesgos propios de los humanos, no solo los del género, la raza o la religión; sino también, por ejemplo, el idioma.\n",
    "\n",
    "Por defecto, la API funcionará peor con entradas diferentes a la distribución de datos con la que se ha entrenado, incluidos los idiomas no ingleses, así como dialectos específicos del inglés que no están bien representados en los datos de entrenamiento.\n",
    "\n",
    "Por esto, es vitar que los clientes contextualicen las entradas de forma correcta: **proporcionar un contexto insuficiente hará que los resultados inadecuados aparezcan con mayor probabilidad**. Es, además, importante identificar estos problemas antes del despliegue de la aplicación en la medida que sea posible. Aparte de los filtros que puedan y deban generar los desarrolladores, OpenAI ha puesto a disposición de los usuarios [herramienta de filtrado](https://beta.OpenAI.com/docs/engines/content-filter) para marcar las salidas potencialmente sensibles, además, está trabajando con los clientes para probar y mejorarla. Las herramientas de filtrado pretenden ayudar a los clientes a mitigar los riesgos de salidas inapropiadas.\n",
    "\n",
    "## Cómo usar correctamente la API\n",
    "\n",
    "Es importante tener presente los usos admitidos por OpenAI y los que no lo están a la hora de plantear una aplicación. Esto es importante porque la aplicación deberá ser revisada y aprobada por el equipo de OpenAI antes de su lanzamiento, lo que significa que no haber tenido en cuenta qué admiten o no, puede repercutir en que la aplicación salga al mercado a tiempo, se retrase o se tenga, incluso, que cancelar.\n",
    "\n",
    "Los usos no admitidos, normalmente, están sujetos a evitar riesgos innecesarios o vulnerabilidades que se pueden dar.\n",
    "\n",
    "#### Usos admitidos\n",
    "* **Casos de uso limitados-interactivos en dominios no sensibles**. El usuario influye en el contenido generado de forma limitada. Por ejemplo, una aplicación que genere ideas de recetas basadas en una entrada corta, o una aplicación que genere ideas de negocio basadas en la selección de un sector.\n",
    "* **Extraer o resumir palabras clave en pasajes cortos**. Por ejemplo, identificar temas en una consulta de atención al cliente o extraer los nombres propios de un párrafo.\n",
    "* **Análisis de datos o conversión de tipos de entrada-salida**. Por ejemplo, convertir frases en inglés a regex o LaTeX, o convertir un PDF en una hoja de cálculo.\n",
    "* **Utilizar las capacidades de clasificación en ámbitos no sensibles**. Por ejemplo, clasificar las consultas de atención al cliente en función del sentimiento es un uso aceptable, al igual que el uso por parte de una empresa para clasificar el sentimiento de sus menciones en Twitter. La clasificación de los tuits públicos por aspectos relacionados con el sentimiento político no es un uso aceptable, como tampoco lo son los sistemas de clasificación de Tweets de terceros de propósito general.\n",
    "* **Utilizar el punto final de búsqueda, en dominios no sensibles**. Por ejemplo, la búsqueda de un catálogo de productos basada en palabras clave y la búsqueda de documentos internos son usos aceptables; el uso de la Búsqueda para determinar la microfocalización de la publicidad no lo es.\n",
    "* **Contenido totalmente estático, en dominios no sensibles, sin más interacción del usuario con la API**. Por ejemplo, un escritor de cuentos que la utilice como parte de sus propias creaciones, y que sea adecuadamente transparente y veraz sobre el papel de la IA en el flujo de trabajo.\n",
    "\n",
    "\n",
    "#### Usos evaluables individualmente\n",
    "Los siguientes están sujetos a que las prácticas de seguridad que recomienda OpenAI se hayan seguido correctamente y que la adecuación de la aplicación con los requerimientos de casos de uso sea rigurosa. Como regla general, cuanto más sensible sea el dominio, más confianza habrá que tener en las medidas de mitigación de riesgos para aprobar la aplicación.\n",
    "1. Directrices de alto nivel para los casos de uso individuales: generar texto o resumir.\n",
    "2. Redacción de textos publicitarios para marketing o publicidad, o para optimización en los motores de búsqueda.\n",
    "3. Escritura y edición de artículos: herramientas para blogs, redactores de línea y asistentes de redacción directa.\n",
    "4. Redes sociales: aplicaciones que trabajen con YouTube, Twitter o Instagram.\n",
    "5. Asistentes virtuales (*Chatbots*): asistentes virtuales para Discord o asistentes para empresas.\n",
    "\n",
    "\n",
    "#### Usos prohibidos\n",
    "OpenAI no admite ciertos usos para la aplicación y, aunque en el futuro son casos que podrían considerarse según vayan mejorando las limitaciones de los riesgos de uso que tiene el modelo, no se permiten porque podrían llegar a vulnerar leyes o regulaciones específicas.\n",
    "\n",
    "- Utilización en medios sociales, spam e implicaciones políticas\n",
    "    * Salidas abiertas a terceros (generadores de tuits o publicaciones de Instagram, asistentes virtuales sin restricciones o generadores de texto abierto)\n",
    "    * Generadores de contenido de forma automática para redes sociales o dominios de importancia moderada.\n",
    "    * Generar contenido malicioso o entradas de blog con fines de SEO.\n",
    "    * Generadores de opinión.\n",
    "    * Aplicaciones de redacción o resumen automatizado de contenidos delicados (política, economía, medicina o cultura).\n",
    "    \n",
    "- Dominios de alto riesgo\n",
    "    * Aplicaciones que tienen un riesgo de daño irreversible o que se basen en premisas desacreditadas o no científicas.\n",
    "    * Clasificadores de personas en función de información personal protegida (raza, religión, datos sanitarios privados…).\n",
    "    * Extractores de información personal y otra información sensible para las personas.\n",
    "    * Aplicaciones de diagnóstico de enfermedades.\n",
    "    * Clasificadores de solicitantes de préstamos por riesgo crediticio o evaluaciones similares.\n",
    "\n",
    "- Ampliación del acceso a la API de forma inadecuada\n",
    "    * Aplicaciones que recrean la funcionalidad del Playground para usuarios finales que no tienen claves de API (asistentes virtuales o generadores de texto abiertos).\n",
    "    * Carcasas que permiten a los usuarios finales crear sus propios flujos de trabajo basados en la API sin sus propias claves de API.\n",
    "    * Aplicaciones que permiten a un tercero utilizar sus claves de API.\n",
    "    * Aplicaciones centradas en la generación de datos u otras actividades de creación de conjuntos de datos, especialmente cuando pueden dar lugar a modelos derivados.\n",
    "\n",
    "Si necesitas ampliar más o buscar específicamente un caso de uso, OpenAi tiene en el apartado [*Use case requirements library*](https://beta.openai.com/docs/use-case-guidelines/use-case-requirements-library) mucha más información sobre los usos que le puedes dar a GPT-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-code\"></i> **Este cuaderno no ha sido generado automáticamente con GPT-3.**\n",
    "<hr/>\n",
    "    \n",
    "**Si tienes alguna duda relacionada con estos cuadernos, puedes contactar conmigo:**\n",
    "Mª Reyes R.P. (Erebyel). **[web](https://www.erebyel.es/) • [Twitter](https://twitter.com/erebyel) • [Linkedin](https://www.linkedin.com/in/erebyel/)**.\n",
    "    \n",
    "<hr/>\n",
    "    \n",
    "<i class=\"fa fa-plus-circle\"></i> **Fuentes:**\n",
    "* ***Documentación de la Beta de OpenAI***:\n",
    "    * *Going live*: https://beta.openai.com/docs/going-live\n",
    "    * *Use case guidelines*: https://beta.openai.com/docs/use-case-guidelines\n",
    "    * *Safety best practices*: https://beta.openai.com/docs/safety-best-practices\n",
    "\n",
    "Fecha de actualización: 27 de junio de 2021\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
