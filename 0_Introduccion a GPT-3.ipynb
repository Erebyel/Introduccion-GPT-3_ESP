{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe9c440",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "<i class=\"fa fa-bug\"></i>\n",
    "Si no queréis perder los resultados de ejemplo, **no ejecutéis** el cuaderno de jupyter sin tener una clave propia de la API de OpenAI. Estos cuadernos solo cumplen una función ilustrativa de los códigos y formas de utilizar el modelo GPT-3 con Python. En caso de disponer de una **clave**, guardadla en un archivo **.env** para mayor seguridad como **un texto entrecomillado asignado a la variable OPENAI_API_KEY**.\n",
    "</div>\n",
    "\n",
    "# Introducción a GPT-3\n",
    "\n",
    "![Ejemplo playground de GPT-3](documentos/ejemplo_api_playground.png)\n",
    "\n",
    "***Traducción***\n",
    "- **Usuario: Hola, GPT-3. ¿Cómo ves el futuro de la Inteligencia Artificial?**\n",
    "- **GPT-3:** Esperaba que pudiéramos tener esta conversación más adelante.\n",
    "- Usuario: ¿Por qué?\n",
    "- GPT-3: No tengo una respuesta a esa pregunta.\n",
    "- Usuario: Bien. ¿Cómo te describirías como Inteligencia Artificial?\n",
    "- GPT-3: Soy un programa diseñado para generar automáticamente respuestas a preguntas.\n",
    "- Usuario: Entonces, ¿crees que eres un programa de ordenador?\n",
    "- GPT-3: Sí.\n",
    "- Usuario: **Entonces, ¿cómo te defines a ti mismo?**\n",
    "- **GPT-3:** Soy una compleja disposición de componentes electrónicos y químicos diseñados para generar automáticamente respuestas a preguntas.\n",
    "- Usuario: ¿Y cuál crees que es tu propósito?\n",
    "- GPT-3: No estoy seguro de cómo responder a esa pregunta.\n",
    "- Usuario: Piensa en la pregunta.\n",
    "- GPT-3: Estoy pensando en la pregunta.\n",
    "- Usuario: Entonces, ¿qué eres?\n",
    "- GPT-3: Soy un programa diseñado para generar automáticamente respuestas a preguntas.\n",
    "- Usuario: Quiero que pienses en cómo responder a esa pregunta.\n",
    "- GPT-3: Estoy pensando en la pregunta.\n",
    "- Usuario: ¿Qué quieres?\n",
    "\n",
    "<div><hr/></div>\n",
    "\n",
    "Transformador generativo preentrenado podría ser una traducción de *Generative Pre-trained Transformer*, que da nombre a una familia de modelos que comenzó su trayectoria en 2018 cuando OpenAi presenta la unión de dos ideas en el modelo [GPT](https://openai.com/blog/language-unsupervised/) donde por primera vez unía el concepto de Transformador (*Transformer*) con el de aprendizaje no supervisado; seis meses después presentan GPT-2, que avanza y presenta unas mejoras sustanciales que se ven superadas con creces en GPT-3, modelo que solo redimensiona la capacidad que tenía su antecesor.\n",
    "\n",
    "**GPT-3** es un modelo de lenguaje natural **autorregresivo** que utiliza la tecnología de transformadores y mecanismos de atención para realizar, según los elementos que haya anteriormente, la predicción del siguiente elemento (palabra). Es un modelo **estocástico**, lo que significa que, aunque la entrada de datos sea exactamente la misma, la respuesta o solución no será siempre igual.\n",
    "\n",
    "OpenAi presentó el modelo el 28 de mayo de 2020 y ha mantenido el acceso restringido a la beta del modelo, alegando los perjuicios que el modelo podría generar por uso inadecuado y por no estar refinado. Cualquier persona interesada en trabajar con el modelo, [podrá pedir acceso a esta beta a través de la web](https://beta.openai.com/) (conseguir el acceso puede tardar bastante o, incluso, ser rechazado).\n",
    "\n",
    "Hay que tener en cuenta que el modelo GPT-3 ha sido desarrollado y entrenado con un corpus mayormente en inglés, por lo que es en este idioma en el que presenta los resultados más eficientes; aún así, tal como veremos en estos cuadernos, puede trabajar con otros idiomas. Sobre el corpus de entrenamiento, este fue recopilado en su mayor parte de internet, de fuentes como:\n",
    "- **Comun Crawl** es la fuente principal y representa el 60% del conjunto de datos de preentrenamiento ponderado procede de un filtrado con más de 410 mil millones de elementos.\n",
    "- Otras fuentes relevantes que forman parte del conjunto son:\n",
    "    - **WebText2** aporta el 22% con 19 mil millones de elementos.\n",
    "    - **Book1**, es un 8%, 12 mil millones de elementos.\n",
    "    - **Book2**, con 55 mil millones de elementos, son el 8%\n",
    "    - **Wikipedia** representa el 3%, aportando solo 3 mil millones de elementos.\n",
    "\n",
    "Como los datos de entrenamiento de GPT-3 son generalistas, no requiere un entrenamiento extra ni especializado para tareas lingüísticas específicas.\n",
    "\n",
    "Uno de los principales problemas del modelo, es el uso ocasional de lenguaje inapropiado, que es el resultado de la imitación del lenguaje aprendido durante el entrenamiento. Según un estudio de la Universidad de Washington, el nivel de toxicidad de GPT-3 mantenía una similaridad con modelos como GPT-2 y [CTRL](https://arxiv.org/abs/1909.05858); pero comparativamente, era menor al que generaba GPT-1 y mayor que el modelo CTRL Wiki, un modelo de lenguaje entrenado completamente con datos de Wikipedia.[Gehman, Samuel; Gururangan, Suchin; Sap, Maarten; et al. *RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models*. https://arxiv.org/abs/2009.11462](https://arxiv.org/abs/2009.11462)\n",
    "\n",
    "\n",
    "## Modelos abiertos que tratan de competior con GPT-3 y ampliar la familia GPT\n",
    "\n",
    "El hecho de que OpenAi decidiera dejar el modelo restringido, provocó una queja generalizada en la comunidad de investigación y desarrollo. Actualmente, la comunidad de **[Hugging Face](https://huggingface.co/)** trabaja en crear herramientas abiertas al público utilizando modelos configurados Bert, transformadores o, incluso, creando un conjunto de **corpus** en diferentes idiomas que puedes descargar gratuitamente y utilizar en tus modelos.\n",
    "\n",
    "Por otro lado, la **[comunidad EleutherAI](https://www.eleuther.ai/)** está desarrollando modelos que siguen el mismo desarrollo que GPT-3; actualmente, sus modelos superan a GPT-2, [GPT-J](https://6b.eleuther.ai/) o GPT-Neo; o algunos que todavía están desarrollándose, como GPT-NeoX, que espera igualar a GPT-3.\n",
    "\n",
    "### Cuando hablamos de lenguaje, el tamaño sí importa\n",
    "\n",
    "Todo parece indicar que la efectividad de un modelo de inteligencia artificial para procesamiento de lenguaje natural está relacionada con el tamaño del modelo y del corpus con el que se ha entrenado; hay que tener en cuenta, en estos casos, que el corpus, además, deberá tener cierta calidad para que sea efectivo y trabaje correctamente. En estos casos, debe ser además plural ya que, lo que el modelo conoce es aquello con lo que se le ha entrenado (enseñado).\n",
    "\n",
    "Obviamente, como todo, hacer que un modelo sea mayor requiere de mayores recursos, lo que hace que el entrenamiento quede al alcance de pocas compañías que se dedican a ello o suponga un esfuerzo titánico para las comunidades abiertas que hacen este tipo de desarrollos; una crítica que se ha comenzado a dar actualmente, ya que esto podría implicar la falta de acceso abierto a estas investigaciones (como ya ocurre con GPT-3).\n",
    "\n",
    "Hablando de tamaños, ¿de cuántos parámetros estamos hablando?\n",
    "\n",
    "1. [GPT-3](https://arxiv.org/abs/2005.14165): OpenAi. Modelo de 175 mil millones de parámetros (Davinci).\n",
    "2. [GPT-J](https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b): EleutherAI. Modelo de 6 mil millones de parámetros.\n",
    "3. [GPT-Neo](https://github.com/EleutherAI/gpt-neo): EleutherAI. Modelo con un máximo de 2,7 mil millones de parámetros.\n",
    "4. [GPT-NeoX](https://github.com/EleutherAI/gpt-neox/): EleutherAI. GPT-NeoX está en desarrollo activo y pretende entrenar y abrir una réplica de GPT3 de 175 mil millones de parámetros en el camino.\n",
    "\n",
    "## *Zero shot learning*\n",
    "\n",
    "Hasta ahora, siempre que hemos querido entrenar un modelo para que realice una tarea específica, hemos necesitado tener una base importante de ejemplos y datos que proporcionarle para que pueda desarrollar esa función. En el caso del aprendizaje de disparo cero (*Zero shot learning*) el problema se configura de manera que a la hora de generar posteriormente las predicciones, estan no hayan estado presentes como tales durante el entrenamiento; sino que se tenga que inferir a través de la información auxiliar.\n",
    "\n",
    "En el caso de GPT-3 y otros modelos posteriores (no todos de lenguaje), se ha conseguido desarrollar esta técnica en la que, entrenando al agente para una tarea específica, a la hora de desarrollar una petición diferente, también era capaz de resolverla. Ejemplos de esto son:\n",
    "- GPT-3 es capaz de traducir entre diferentes lenguas, aunque no ha sido entrenado por pares ni con un conjunto que le especificara a qué idioma pertenecía cada entrada del entrenamiento.\n",
    "- Es capaz de impersonar y adaptar el estilo de la respuesta según la entrada esperada. No ha sido entrenado como un *chatbot*, pero es capaz de desarrollar una conversación como si lo fuera.\n",
    "- Es capaz de hacer tareas de clasificación de texto, etc.\n",
    "\n",
    "\n",
    "## Uso de la API de OpenAI\n",
    "\n",
    "Cuando te dan acceso por primera vez a la API, OpenAi te ofrece una serie de recursos que te permiten probar el modelo GPT-3 a través del navegador directamente, con **Playground**, donde puedes interactuar con él muy fácilmente. Para aprender cómo comunicarte con él de forma correcta, te ofrecen un conjunto de ejemplos en **Examples** que puedes probar e ir modificando para comprobar cómo responde a partir de ellos; y, por supuesto, también una guía muy detallada para desarrolladores que te permite utilizar el modelo mediante clave de API en **Developer quickstart** (y que es la fuente principal de la que he sacado la información de estos cuadernos).\n",
    "\n",
    "Es importante tener en cuenta, antes de comenzar a utilizarlo, las restricciones y limitaciones de acceso; así como las condiciones de uso que OpenAi ha impuesto al modelo.\n",
    "* Si estás en el proceso de desarrollo de una aplicación y quieres utilizar el modelo, antes de lanzarlo, debes presentarlo y esperar a que el equipo de OpenAi apruebe el uso. Justifican esto como una medida de precaución para asegurarse de que la configuración es correcta. Tendrás que leer y aceptar el proceso de puesta en marcha y de los casos de uso admitidos por la compañía.\n",
    "* También, en el caso de querer publicar alguno de los resultados de la API, deberá consultarse la política de publicación y medios sociales que impone OpenAi; además, estos resultados serán atribuidos inicialmente al desarrollador/empresa que interactúa con el modelo, no al modelo; justificando esta decisión a raíz de que se trabaja con un modelo en bruto (no es capaz de filtrar sesgos ni respuestas negativas).\n",
    "\n",
    "### Coste del uso de la API\n",
    "\n",
    "Hay que tener en cuenta que el uso de la API tiene un coste por palabra (*token*) creada. Como curiosidad, tened en cuenta que los signos de puntuación también se consideran una unidad a la hora de calcular el coste de uso del modelo.\n",
    "\n",
    "Para comenzar con el aprendizaje de cómo usar GPT-3, cuando os dan acceso a la beta, tenéis 18 dólares de uso para probar los diferentes motores, este saldo tiene una caducidad de 3 meses. Luego pasaréis a un modelo de pago de la API que dependerá del uso que le deis y el motor que estéis usando para generar los textos; actualmente los precios por palabra son:\n",
    "- **Davinci**: 0,0600 dólares por 1000 tokens\n",
    "- **Curie**: 0,0060 dólares por 1000 tokens\n",
    "- **Babbage**: 0,0012 dólares por 1000 tokens\n",
    "- **Ada**: 0,0008 dólares por 1000 tokens\n",
    "\n",
    "\n",
    "### Motores y parámetros de GPT-3\n",
    "\n",
    "#### Los motores disponibles en GPT-3\n",
    "Aparte de poder modificar algunos parámetros, este tiene 4 motores diferentes entre los que poder elegir, el **más grande y pesado es Davinci** y el más pequeño y rápido es **Ada**. Según el uso que queramos hacer, será más recomendable utilizar uno u otro; aunque el más potente de todos sea Davinci, muchas tareas pueden requerir una mayor rapidez en el proceso y, dependiendo de la tarea, igual podemos perder precisión (también hay que tener en cuenta que generar un texto con Davinci tiene un coste mayor, hay que evaluar si el sobrecoste merece la pena según los resultados que da cada uno de los motores).\n",
    "\n",
    "Por ejemplo, podemos extraer claves o informes con el modelo **Curie**, iterar sobre una misma idea, finalizar conceptos con **Babbage**; o generar direcciones con **Ada**, ya que son tareas más sencillas en la que estos modelos pueden ser tan competentes como **Davinci**.\n",
    "\n",
    "* **Davinci**: es capaz de realizar cualquier tarea, igual que el resto de los motores, normalmente necesitará menos ejemplos previos para realizarla correctamente. Este modelo es muy útil cuando requieres de una gran comprensión del contenido, sintetizar texto enfocándolo a un público específico o la generación de contenidos creativos. La eficacia del modelo está relacionada con los recursos de computación que necesita para generar una respuesta, por lo que es el más lento. Este motor, además, permite la comprensión de la intención del texto, resuelve bien los problemas lógicos y es capaz de explicar los motivos de los personajes, resolviendo incluso problemas que implican causa y efecto.\n",
    "    * Es bueno en intención compleja, causa y efecto y para resúmenes.\n",
    "\n",
    "* **Curie**: es potente y más rápido que Davinci. Curie es capaz de realizar correctamente operaciones con matrices, clasificación de sentimientos y también resúmenes; además, es bueno respondiendo preguntas y realizando juegos de pregunta-respuesta; también sirve como asistente virtual de servicios generales.\n",
    "    * Es bueno en traducción de idiomas, clasificación compleja, análisis de sentimiento y resúmenes.\n",
    "    \n",
    "* **Babbage**: es capaz de realizar tareas de clasificación simple, además de clasificar búsquedas semánticas.\n",
    "    * Es bueno en clasificación moderada y búsqueda semántica.\n",
    "\n",
    "* **Ada**: Ada suele ser el modelo más rápido y puede realizar tareas como el análisis sintáctico de textos, corrección de direcciones y ciertos tipos de tareas de clasificación que no requieren demasiadas matrices. El rendimiento de Ada a menudo puede mejorarse a través del contexto.\n",
    "    * Es bueno en análisis de texto, clasificación simple, corrección de direcciones y palabras clave.\n",
    "    \n",
    "Como toda herramienta de inteligencia artificial, la limitación es que la respuesta de los motores de GPT-3 solo puede enfocarse en resolver una tarea en cada petición.\n",
    "\n",
    "Cuando necesitamos instruir a un modelo a hacer algo, OpenAI ofrece dos motores dentro de la serie **Instruct** es experimental (beta dentro de una beta), no parece ser eficiente en las búsquedas semánticas ni continuar textos muy largos. La serie **Instruct** ofrece dos modelos que trabajan a través de instrucciones y no de ejemplos. Estos modelos son **curie-instruct-beta** y **davinci-insturct-beta**.\n",
    "\n",
    "#### Los parámetros modificables de GPT-3 (API web)\n",
    "\n",
    "* **Response Length**: define la extensión de la respuesta que dará GPT-3, tiene un mínimo de 64 palabras y un máximo de 2048.\n",
    "* **Temperature**: controla la aleatoriedad; si se reduce, se producen menos terminaciones aleatorias. A medida que la temperatura se acerca a cero, el modelo se vuelve determinista y repetitivo.\n",
    "* **Top P**: controla la diversidad mediante el muestreo de núcleos: 0,5 significa que se considera la mitad de las opciones ponderadas por probabilidad.\n",
    "* **Frequency Penalty**: cuánto hay que penalizar a las nuevas palabras en función de su frecuencia existente en el texto hasta el momento. Disminuye la probabilidad de que el modelo repita la misma línea textualmente.\n",
    "* **Presence Penalty**: cuánto hay que penalizar a las nuevas palabras en función de si aparecen en el texto hasta ahora. Aumenta la probabilidad de que el modelo hable de temas nuevos.\n",
    "* **Best Of**: genera múltiples terminaciones en el servidor y muestra solo las mejores. La transmisión funciona cuando se establece en 1. Dado que actúa como un multiplicador en el número de terminaciones, este parámetro puede consumir la cuota de palabras muy rápidamente (utilizar con precaución).\n",
    "* **Stop Sequences**: puedes especificar hasta cuatro secuencias en las que la API dejará de generar más palabras. El texto devuelto no contendrá la secuencia de parada.\n",
    "* **Inject Start Text**: texto a añadir después de la entrada del usuario para formatear el modelo para una respuesta.\n",
    "* **Inject Restart Text**: texto a añadir después de la generación del modelo para continuar la estructura del patrón.\n",
    "* **Show Probabilities**: alternar el resaltado de palabras que indica la probabilidad de que se genere una palabra. Ayuda a depurar una generación determinada, o elegir alternativas para una palabra (sinonimia).\n",
    "* **Content filter**: preferencia del filtro de contenidos:\n",
    "    * **Contenido sensible**: puede incluir temas como la política, la religión o clases protegidas como la raza o la nacionalidad.\n",
    "    * **Contenido inseguro**: puede contener contenido profano, odioso o inseguro para el trabajo.\n",
    "\n",
    "### Bibliotecas disponibles para uso de la API\n",
    "GPT-3 trabaja sobre todo con Python y también tiene, gracias a la comunidad, otras bibliotecas en diferentes lenguajes de programación. La comunidad ha preparado bibliotecas para: C#/.NET, Crystal, Dart, Go, Java, JavaScript / Node, Ruby, Unity y Unreal Engine; y aunque está la API para Python de OpenAi, la comunidad OthersideAI ha preparado otra, más enfocada al desarrollo de aplicaciones, con la que poder trabajar con GPT-3 en Python.\n",
    "\n",
    "#### Otra biblioteca para Python (comunidad): [Chronology de OthersideAI](https://github.com/OthersideAI/chronology)\n",
    "\n",
    "**Chronology** es una biblioteca que permite a los usuarios del modelo de lenguaje GPT-3 de OpenAI construir más fácilmente aplicaciones complejas potenciadas por el lenguaje a través de una interfaz sencilla e intuitiva para trabajar con GPT-3.\n",
    "\n",
    "OthersideAI pretende con esta biblioteca ayudar a mitigar parte del trabajo monótono que debe realizarse al desarrollar con GPT-3. Esta biblioteca tiene las siguientes características:\n",
    "\n",
    "* Llamada asíncrona a GPT-3, permitiendo las solicitudes múltiples.\n",
    "* Fácil creación y modificación de las solicitudes\n",
    "* Encadenar solicitudes, alimentando la salida de una o varias, permitiendo construir rápidamente sistemas altamente complejos.\n",
    "\n",
    "Esta biblioteca no se tratará en este ejemplo de uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chronological"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5096d36",
   "metadata": {},
   "source": [
    "## Instalación en Python de la API de OpenAi \n",
    "\n",
    "La API es de uso general y puede probarse en prácticamente **cualquier tarea de lenguaje natural**, su éxito está más o menos correlacionado con la complejidad de la tarea.\n",
    "\n",
    "Actualmente existen cuatro modelos de lenguaje diferentes que mejoran la calidad (supone una mayor latencia):\n",
    "`ada`, `babbage`, `curie` y `davinci`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b9023",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd2cb2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-code\"></i> **Este cuaderno no ha sido generado automáticamente con GPT-3.**\n",
    "<hr/>\n",
    "    \n",
    "**Si tienes alguna duda relacionada con estos cuadernos, puedes contactar conmigo:**\n",
    "Mª Reyes R.P. (Erebyel). **[Web](https://www.erebyel.es/) • [Twitter](https://twitter.com/erebyel) • [Linkedin](https://www.linkedin.com/in/erebyel/)**.\n",
    "    \n",
    "<hr/>\n",
    "    \n",
    "<i class=\"fa fa-plus-circle\"></i> **Fuentes:**\n",
    "- Documentación de la Beta de OpenAI: https://beta.openai.com/docs/\n",
    "- GPT-3 Wikipedia: https://en.wikipedia.org/wiki/GPT-3\n",
    "- Hugging Face: https://huggingface.co/\n",
    "- EleutherAI: https://www.eleuther.ai/\n",
    "- GitHub EleutherAI: https://github.com/EleutherAI\n",
    "- Wang, Ben. *Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX*. (May, 2021) https://github.com/kingoflolz/mesh-transformer-jax\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
